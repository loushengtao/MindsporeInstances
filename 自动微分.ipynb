{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mindspore自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动微分能够计算可导函数在某点处的导数值，是反向传播算法的一般化。自动微分主要解决的问题是将一个复杂的数学运算分解为一系列简单的基本运算，该功能对用户屏蔽了大量的求导细节和过程，大大降低了框架的使用门槛。\n",
    "\n",
    "MindSpore使用**ops.GradOperation**计算一阶导数，**ops.Gradoperation**的示例如下：\n",
    "$$f(x)=\\omega x +b$$\n",
    "\n",
    "可以构造一个Net类，w取6，b取1\n",
    "\n",
    "$$f(x)=6x+1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.]\n"
     ]
    }
   ],
   "source": [
    "import mindspore as ms\n",
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "\n",
    "class Net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.w=ms.Parameter(ms.Tensor(np.array([6],np.float32)),name='w')\n",
    "        self.b=ms.Parameter(ms.Tensor(np.array([1],np.float32)),name='b')\n",
    "    \n",
    "    def construct(self,x):\n",
    "        f=self.w*x+self.b\n",
    "        return f\n",
    "\n",
    "gs=Net() # 创建公式实例\n",
    "print(gs(2))  # 传入x的值，进行推理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mindspore里的construct类似于forward，可以用前向调用。可以看到当我们传入x等于2时候，值为14，公式已经构建完成了，接下来就是进行自动微分的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义求导类GradNet\n",
    "\n",
    "* 求导类的__init__中定义需要求导的网络self.net和ops.GradOperation操作\n",
    "\n",
    "* 求导类的construct函数中对self.net进行求导\n",
    "\n",
    "$$f^{'}(x)=\\omega$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeadNode\n"
     ]
    }
   ],
   "source": [
    "import mindspore.ops as ops\n",
    "from numpy import gradient\n",
    "\n",
    "class GradNet(nn.Cell):\n",
    "    def __init__(self,net):\n",
    "        super(GradNet,self).__init__()\n",
    "        self.net=net\n",
    "        self.grad_op=ops.GradOperation()\n",
    "    \n",
    "    def construct(self,x):\n",
    "        gradient_func=self.grad_op(self.net)\n",
    "        return gradient_func\n",
    "\n",
    "x=ms.Tensor([100],dtype=ms.float32)\n",
    "qd=GradNet(Net())(x)\n",
    "print(qd)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处输出DeadNode，是因为我们返回的是导函数而不是导函数值，return gradient_func应改为return gradient_func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.]\n"
     ]
    }
   ],
   "source": [
    "import mindspore.ops as ops\n",
    "from numpy import gradient\n",
    "\n",
    "class GradNet(nn.Cell):\n",
    "    def __init__(self,net):\n",
    "        super(GradNet,self).__init__()\n",
    "        self.net=net\n",
    "        self.grad_op=ops.GradOperation()\n",
    "    \n",
    "    def construct(self,x):\n",
    "        gradient_func=self.grad_op(self.net)\n",
    "        return gradient_func(x)\n",
    "\n",
    "x=ms.Tensor([100],dtype=ms.float32)\n",
    "qd=GradNet(Net())(x)\n",
    "print(qd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对权重求一阶导\n",
    "\n",
    "在很多场景之中，一个函数式往往包含多个参数，权重也有极大一部分是可学习的参数。故有时候我们也需要对权重进行求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 对权重参数求一阶导，需要将ops.GradOperation中的get_by_list设置为True(默认为False)\n",
    "\n",
    "* 若某些权重不需要进行求导，则在定义求导网络时，相应的参数声明定义的时候将其requires_grad属性设置为False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "\n",
    "class GradNet(nn.Cell):\n",
    "    def __init__(self,net):\n",
    "        super(GradNet,self).__init__()\n",
    "        self.net=net\n",
    "        self.params=ms.ParameterTuple(net.trainable_params())\n",
    "        self.grad_op=ops.GradOperation(get_by_list=True)\n",
    "\n",
    "    def construct(self,x):\n",
    "        gradient_func=self.grad_op(self.net,self.params)\n",
    "        return gradient_func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里介绍一下ops.GradOperation的一些输入参数：\n",
    "\n",
    "* **get_all**：计算梯度，如果等于False，获得第一个输入的梯度，如果等于True\n",
    "\n",
    "* **get_by_list**：是否对权重的参数进行求导，默认值为False\n",
    "\n",
    "* **sens_param**：是否对网络的输出值做缩放以改变最终梯度，默认值为False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，对函数进行求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tensor(shape=[1], dtype=Float32, value= [1.00000000e+002]), Tensor(shape=[1], dtype=Float32, value= [1.00000000e+000]))\n",
      "wgrad:[100.],\n",
      "bgrad[1.]\n"
     ]
    }
   ],
   "source": [
    "x=ms.Tensor([100],dtype=ms.float32)\n",
    "ds=GradNet(Net())(x)\n",
    "\n",
    "print(ds)\n",
    "\n",
    "print(f\"wgrad:{ds[0]},\\nbgrad{ds[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么如果当某一个某一个权重不需要求导，我们只需在创建该权重参数的地方将其**requires_grad**属性改为False即可"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f21bb2d5c565dbc33d815445cee0d4e3f2f7951520fd8c0e3b4200672f41bfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
